{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "from tensorflow.python.layers.base import Layer\n",
    "import svhn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "norm = mpl.colors.Normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"matplotlib.image\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn=svhn_util.read_data_sets('svhn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=32\n",
    "height=32\n",
    "channel=3\n",
    "Gaussian_scale=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,height*width*channel],name='input_image')\n",
    "y=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "\n",
    "mask_psed_neg=tf.reshape(tf.cast(tf.equal(y,1),tf.float32),(-1,1))\n",
    "mask_not_psed_neg=tf.reshape(tf.cast(tf.logical_not(tf.equal(y,1)),tf.float32),(-1,1))\n",
    "\n",
    "y_onehot=tf.one_hot(y,3)\n",
    "\n",
    "# class CNN_Classifier(Layer):\n",
    "#     def __init__(self):\n",
    "#         self.conv1=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv1',_reuse=tf.AUTO_REUSE)\n",
    "#         self.conv2=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv2',_reuse=tf.AUTO_REUSE)\n",
    "#         self.conv3=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv3',_reuse=tf.AUTO_REUSE)\n",
    "#         self.max_pool=tf.layers.MaxPooling2D(pool_size=2,strides=2,_reuse=tf.AUTO_REUSE)\n",
    "#         self.avg_pool=tf.layers.AveragePooling2D(pool_size=4,strides=4,_reuse=tf.AUTO_REUSE)\n",
    "#         self.fc_hidden=tf.layers.Dense(units=512,name='fc_hidden',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "#         self.fc=tf.layers.Dense(units=3,name='output_linear',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "#     def __call__(self,inputs):\n",
    "#         out1=self.conv1(inputs)\n",
    "#         out2=self.conv2(out1)\n",
    "#         out3=self.conv3(out2)\n",
    "#         out3=tf.layers.flatten(out3)\n",
    "#         score=self.fc(self.fc_hidden(out3))\n",
    "#         return score\n",
    "    \n",
    "class CNN_Classifier(Layer):\n",
    "    def __init__(self):\n",
    "        self.conv1=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv1',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv2=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv2',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv3=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv3',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv4=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv4',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv5=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv5',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv6=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv6',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv7=tf.layers.Conv2D(filters=64,kernel_size=5,strides=2,padding='same',activation=tf.nn.relu,name='conv7',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv8=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv8',_reuse=tf.AUTO_REUSE)\n",
    "        self.conv9=tf.layers.Conv2D(filters=32,kernel_size=3,strides=1,padding='same',activation=tf.nn.relu,name='conv9',_reuse=tf.AUTO_REUSE)\n",
    "        self.max_pool=tf.layers.MaxPooling2D(pool_size=2,strides=2,_reuse=tf.AUTO_REUSE)\n",
    "        self.avg_pool=tf.layers.AveragePooling2D(pool_size=4,strides=4,_reuse=tf.AUTO_REUSE)\n",
    "        self.fc_hidden=tf.layers.Dense(units=512,name='fc_hidden',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.fc=tf.layers.Dense(units=3,name='output_linear',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    def __call__(self,inputs):\n",
    "#         inputs=self.conv3(self.conv2(self.conv1(inputs)))\n",
    "#         inputs=self.conv6(self.conv5(self.conv4(inputs)))\n",
    "#         inputs=self.conv9(self.conv8(self.conv7(inputs)))\n",
    "        inputs=self.conv7(self.conv4(self.conv1(inputs)))\n",
    "        inputs=tf.layers.flatten(inputs)\n",
    "        score=self.fc(self.fc_hidden(inputs))\n",
    "        return score\n",
    "\n",
    "class NN_Classifier(Layer):\n",
    "    def __init__(self):\n",
    "        self.linear1=tf.layers.Dense(units=512,name='linear1',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear2=tf.layers.Dense(units=512,name='linear2',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear3=tf.layers.Dense(units=512,name='linear3',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear4=tf.layers.Dense(units=512,name='linear4',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear5=tf.layers.Dense(units=512,name='linear5',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear6=tf.layers.Dense(units=512,name='linear6',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear7=tf.layers.Dense(units=512,name='linear7',activation=tf.nn.relu,_reuse=tf.AUTO_REUSE)\n",
    "        self.linear8=tf.layers.Dense(units=3,name='linear8',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    def __call__(self,inputs):\n",
    "        score=self.linear4(self.linear3(self.linear2(self.linear1(inputs))))\n",
    "        score=self.linear8(self.linear7(self.linear6(self.linear5(score))))\n",
    "#         score=self.linear8(self.linear1(inputs))\n",
    "        return score\n",
    "    \n",
    "class Encoder_conv(Layer):\n",
    "    def __init__(self):\n",
    "        self.encoder_low1=tf.layers.Conv2D(filters=32,kernel_size=9,activation=tf.nn.relu,name='encoder_low1',_reuse=tf.AUTO_REUSE)\n",
    "        self.encoder_low2=tf.layers.Conv2D(filters=32,kernel_size=9,activation=tf.nn.relu,name='encoder_low2',_reuse=tf.AUTO_REUSE)\n",
    "        self.encoder_low3=tf.layers.Conv2D(filters=32,kernel_size=9,activation=tf.nn.relu,name='encoder_low3',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    def __call__(self,inputs):\n",
    "        encode_hid=self.encoder_low3(self.encoder_low2(self.encoder_low1(inputs)))\n",
    "        encode_hid=tf.layers.flatten(encode_hid)\n",
    "        return encode_hid\n",
    "    \n",
    "class Decoder_conv(Layer):\n",
    "    def __init__(self):\n",
    "        self.decoder_top=tf.layers.Dense(units=2048,name='decoder',_reuse=tf.AUTO_REUSE)\n",
    "        self.decoder_low3=tf.layers.Conv2DTranspose(32,9,activation=tf.nn.relu,name='decoder_low3',_reuse=tf.AUTO_REUSE)\n",
    "        self.decoder_low2=tf.layers.Conv2DTranspose(32,9,activation=tf.nn.relu,name='decoder_low2',_reuse=tf.AUTO_REUSE)\n",
    "        self.decoder_low1=tf.layers.Conv2DTranspose(3,9,name='decoder_low1',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    def __call__(self,inputs):\n",
    "        decode_hid=self.decoder_top(inputs)\n",
    "        decode_hid=tf.reshape(decode_hid,shape=[-1,8,8,32])\n",
    "        decode_hid=self.decoder_low3(decode_hid)\n",
    "        decode_hid=self.decoder_low2(decode_hid)\n",
    "        decode_hid=self.decoder_low1(decode_hid)\n",
    "        return tf.layers.flatten(decode_hid)\n",
    "    \n",
    "class Decoder_NN(Layer):\n",
    "    def __init__(self):\n",
    "        self.decode_low=tf.layers.Dense(units=512,activation=tf.nn.relu,name='emit_hidden_1',_reuse=tf.AUTO_REUSE)\n",
    "        self.decode_top=tf.layers.Dense(units=height*width*channel,activation=tf.nn.relu,name='emit_net',_reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    def __call__(self,inputs):\n",
    "        return self.decode_top(self.decode_low(inputs))\n",
    "    \n",
    "class VAE(Layer):\n",
    "    def __init__(self):\n",
    "        self.lstm_hidden_dim=128\n",
    "        self.latent_dim=10\n",
    "        self.n_time=15\n",
    "        self.encoder_lstm=tf.contrib.rnn.GRUCell(self.lstm_hidden_dim)\n",
    "        self.decoder_lstm=tf.contrib.rnn.GRUCell(self.lstm_hidden_dim)\n",
    "        self.encode_mean=tf.layers.Dense(units=self.latent_dim,name='encode_mean')   # calculate mean of z\n",
    "        self.encode_std=tf.layers.Dense(units=self.latent_dim,name='encode_std')   # calculate std of z\n",
    "        self.unit_gaussian=tf.distributions.Normal(loc=tf.zeros(self.latent_dim),scale=Gaussian_scale*tf.ones(self.latent_dim))\n",
    "        self.emit_net=tf.layers.Dense(units=height*width*channel,name='emit_net')\n",
    "#         self.emit_net=Decoder_NN()\n",
    "        \n",
    "        \n",
    "    def __call__(self,inputs):\n",
    "        batch_size=tf.shape(inputs)[0]\n",
    "        state_encode=self.encoder_lstm.zero_state(batch_size, tf.float32)   # lstm tuple (c,h)\n",
    "        state_decode=self.decoder_lstm.zero_state(batch_size, tf.float32)   # lstm tuple (c,h)\n",
    "        original_image=tf.layers.flatten(inputs)\n",
    "        output_image=None\n",
    "        latent_loss_his=[]\n",
    "        classifier_loss_his=[]\n",
    "        likelihood_his=[]\n",
    "        predict_his=[]\n",
    "        image_his=[]\n",
    "        \n",
    "        for t in range(self.n_time):\n",
    "            if t==0:\n",
    "                image=original_image\n",
    "            else:\n",
    "                # use output_image for fake images, use original image for real image\n",
    "                image=mask_psed_neg*output_image+mask_not_psed_neg*original_image\n",
    "            image_his.append(image)\n",
    "            '''\n",
    "            Part I: Classifier\n",
    "            '''\n",
    "            classifier_scope=\"CNN_classifier_\"+str(t)\n",
    "            input_image=tf.reshape(image,(-1,height*width*channel))\n",
    "#             input_image=tf.reshape(image,(-1,height,width,channel))\n",
    "            with tf.variable_scope(classifier_scope) as vs:\n",
    "#                 classifier=CNN_Classifier()\n",
    "                classifier=NN_Classifier()\n",
    "                score_likelihood=classifier(input_image)\n",
    "            with tf.variable_scope(classifier_scope) as vs:\n",
    "#                 classifier=CNN_Classifier()\n",
    "                classifier=NN_Classifier()\n",
    "                score_classifier=classifier(tf.stop_gradient(input_image))\n",
    "            \n",
    "            score_likelihood=tf.reshape(score_likelihood[:,0]-score_likelihood[:,1],(-1,1))*mask_psed_neg\n",
    "            likelihood_his.append(score_likelihood) # only record score for fake images\n",
    "            classifier_loss_his.append(tf.losses.softmax_cross_entropy(logits=score_classifier,onehot_labels=y_onehot))\n",
    "            predict_his.append(score_classifier)\n",
    "            \n",
    "            '''\n",
    "            Part II: Recurrent VAE\n",
    "            '''\n",
    "#             with tf.variable_scope(\"pre_encoder\") as vs:\n",
    "#                 preencode_net=Encoder_conv()\n",
    "#                 image=preencode_net(tf.reshape(image,(-1,height,width,channel)))\n",
    "            with tf.variable_scope(\"LSTM_encoder\") as vs:\n",
    "                encode_out,state_encode=self.encoder_lstm(image,state_encode)\n",
    "            z_mean=self.encode_mean(encode_out)\n",
    "            z_std=tf.exp(self.encode_std(encode_out))\n",
    "            distrib_encode=tf.distributions.Normal(loc=z_mean,scale=z_std)\n",
    "            Z=distrib_encode.sample()\n",
    "            \n",
    "            latent_loss=tf.reduce_sum(mask_psed_neg*tf.distributions.kl_divergence(distrib_encode,self.unit_gaussian),axis=1)\n",
    "#             latent_loss=tf.reduce_sum(tf.square(z_mean),axis=1)+tf.reduce_sum(tf.square(z_std-1),axis=1)\n",
    "            latent_loss_his.append(latent_loss)\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM_decoder\") as vs:\n",
    "                decode_out,state_decode=self.decoder_lstm(Z,state_decode)\n",
    "                \n",
    "            \n",
    "            output_image=tf.nn.sigmoid(self.emit_net(decode_out))-0.5\n",
    "            \n",
    "        tot_latent_loss=tf.reduce_mean(latent_loss_his)\n",
    "        tot_classifier_loss=tf.reduce_mean(classifier_loss_his)\n",
    "        tot_likelihood_loss=tf.reduce_sum(likelihood_his,axis=0)  # accumulate score through time\n",
    "        tot_likelihood_loss=tf.reduce_mean(-tot_likelihood_loss)\n",
    "        \n",
    "        tot_loss=tot_latent_loss+tot_classifier_loss+tot_likelihood_loss\n",
    "            \n",
    "        return image_his,tot_loss,tot_latent_loss,tot_classifier_loss,tot_likelihood_loss,predict_his\n",
    "\n",
    "\n",
    "vae=VAE()\n",
    "image_his,tot_loss,tot_latent_loss,tot_classifier_loss,tot_likelihood_loss,predict_his=vae(X)\n",
    "tot_classifier_loss=tot_classifier_loss\n",
    "tot_loss2=tot_latent_loss+tot_likelihood_loss\n",
    "\n",
    "classifier_variables=[]\n",
    "vae_variables=[]\n",
    "for v in tf.global_variables():\n",
    "    if v.name.find('CNN_classifier')!=-1:\n",
    "        classifier_variables.append(v)\n",
    "    else:\n",
    "        vae_variables.append(v)\n",
    "\n",
    "\n",
    "learning_rate1 = 1e-3\n",
    "optimizier1=tf.train.AdamOptimizer(learning_rate=learning_rate1)\n",
    "train_step1 = optimizier1.minimize(tot_classifier_loss,var_list=classifier_variables)\n",
    "\n",
    "learning_rate2 = 1e-3\n",
    "optimizier2=tf.train.AdamOptimizer(learning_rate=learning_rate2)\n",
    "train_step2 = optimizier2.minimize(tot_loss2,var_list=vae_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate process\n",
    "G_batch_size=tf.placeholder(shape=(),dtype=tf.int32,name='batch_size')\n",
    "G_state_decode=vae.decoder_lstm.zero_state(G_batch_size, tf.float32)\n",
    "G_unit_gaussian=tf.distributions.Normal(loc=tf.zeros((G_batch_size,vae.latent_dim)),scale=Gaussian_scale*tf.ones((G_batch_size,vae.latent_dim)))\n",
    "\n",
    "G_output_his=[]\n",
    "\n",
    "for t in range(vae.n_time):\n",
    "    G_Z=G_unit_gaussian.sample()\n",
    "    with tf.variable_scope(\"LSTM_decoder\") as vs:\n",
    "        G_decode_out,G_state_decode=vae.decoder_lstm(G_Z,G_state_decode)\n",
    "    G_output_image=vae.emit_net(G_decode_out)\n",
    "    G_output_image=tf.nn.sigmoid(G_output_image)-0.5\n",
    "    G_output_his.append(G_output_image)\n",
    "G_output_result=G_output_his[-2]   # last image is mearningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display():\n",
    "    output_images=sess.run(G_output_result,feed_dict={G_batch_size:9})\n",
    "    output_images=output_images+0.5\n",
    "#     output_images=(output_images+0.1)/0.2\n",
    "    \n",
    "    plt.figure(figsize = (3,3))\n",
    "    gs1 = gridspec.GridSpec(3,3)\n",
    "    gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "    for i in range(9):\n",
    "        plt.subplot(gs1[i])\n",
    "        norm.autoscale(output_images[i,:])\n",
    "        img=norm(output_images[i,:])\n",
    "        plt.imshow(img.reshape(32,32,3))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    test=sess.run(G_output_result,feed_dict={G_batch_size:64})\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size,fake_size,noise_size):\n",
    "    X_true=svhn.next_batch(batch_size)\n",
    "    X_true=X_true-0.5  # range=(-0.5,0.5)\n",
    "#     X_true=X_true*0.2-0.1  # range=(-0.5,0.5)\n",
    "    X_fake=np.random.uniform(low=-0.7,high=0.7,size=(fake_size,height*width*channel))\n",
    "    X_noise=np.random.uniform(low=-0.7,high=0.7,size=(noise_size,height*width*channel))\n",
    "    image=np.vstack((X_true,X_fake,X_noise))\n",
    "    label=np.concatenate((np.zeros(batch_size),np.ones(fake_size),2*np.ones(noise_size)))\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "fake_size=128\n",
    "noise_size=512\n",
    "num_iteration=20000\n",
    "print_every=50\n",
    "\n",
    "\n",
    "classifier_train=1\n",
    "vae_train=1\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     saver.restore(sess, \"parameters/IVRAE/IVRAE.ckpt\")\n",
    "    for it in range(num_iteration):\n",
    "        image,label=make_batch(batch_size,fake_size,noise_size)\n",
    "        feed_dict={X:image,y:label}\n",
    "        for t in range(classifier_train):\n",
    "            _=sess.run([train_step1],feed_dict={X:image,y:label})\n",
    "        for t in range(vae_train):\n",
    "            _=sess.run([train_step2],feed_dict={X:image,y:label})\n",
    "        loss_num,l1,l2,l3=sess.run([tot_loss,tot_latent_loss,tot_classifier_loss,tot_likelihood_loss],feed_dict=feed_dict)\n",
    "        \n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'classifier loss = %f, likelihood loss = %f = (%f+%f)' % (l2,l1+l3,l1,l3))\n",
    "            save_path = saver.save(sess, \"parameters/IVRAE/IVRAE.ckpt\")\n",
    "        \n",
    "            gene=display()\n",
    "            \n",
    "            temp=sess.run(image_his,feed_dict={X:image,y:label})\n",
    "            temp=temp[-1]\n",
    "            plt.scatter(temp[:batch_size,1000],temp[:batch_size,1003],c='r')\n",
    "            plt.scatter(temp[batch_size:(batch_size+fake_size),1000],temp[batch_size:(batch_size+fake_size),1003],c='b')\n",
    "            # plt.scatter(temp[-noise_size:,1000],temp[-noise_size:,1003],c='g')\n",
    "            plt.scatter(gene[:,1000],gene[:,1003],c='g')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
